---
title: 2.requst请求库
cover: 'https://s2.loli.net/2023/06/24/jR4g5LzpoJYFbD9.png'
tags: requst请求库
categories: 爬虫
top_img: 'https://s2.loli.net/2023/06/24/jR4g5LzpoJYFbD9.png'
abbrlink: 936f1956
---
### requst请求库

常⽤的抓取⻚⾯的模块通常使⽤⼀个第三⽅模块requests， 这个模块的优势就是⽐urllib还要简单，并且处理各种请求都⽐较⽅便。

既然是第三⽅模块，那就需要我们对该模块进⾏安装，安装⽅法:

```
pip install requests
```

如果安装速度慢的话可以改⽤国内的源进⾏下载安装.

```shell
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests
```

![图片](https://s2.loli.net/2023/06/13/f7c1TSnUuBrDlbw.png)



OK. 接下来我们来看看requests能带给我们什么？我们先请求一下百度首页，一般的案例都是从这里开始的，俗称梦开始的地方：

## 案例1：请求百度首页源代码

```python
import requests
url = "https://www.baidu.com/"
response = requests.get(url)
response.encoding = "utf-8"
print(response.text)
```

![图片](https://s2.loli.net/2023/06/13/LZ6cpy4Xwn7diTs.png)



这样运行的话就把百度首页的html提取出来了。

## 案例2：搜狗搜索词，交互式返回结果

```shell
import requests
kw = input("请输入需要查询的内容")

url = f"https://www.sogou.com/web?query={kw}"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"
}
response = requests.get(url,headers = headers)
print(response.text)  # 直接拿结果(⽂本)

with open("./sougou.html","w",encoding="utf-8") as f:
    f.write(response.text)
```



案例3：提取百度翻译结果



接下来, 我们看⼀个稍微复杂那么⼀丢丢的, 百度翻译~注意百度翻译这个url不好弄出来. 记住, 在输⼊的时候, 关掉各种输⼊法，要⽤英⽂输⼊法, 然后不要回⻋. 就能看到这个sug了

![图片](https://s2.loli.net/2023/06/13/BG8edcREmfPgK56.png)



```python
import requests

url = "https://fanyi.baidu.com/sug"
# 请注意百度翻译的sug这个url. 它是通过post⽅式进⾏提交的. 所以我们也要模拟post请求
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3884.400 QQBrowser/10.8.4560.400'
}
word = input("请输入需要查询的词")
data = {
    'kw':word  # 这⾥要和抓包⼯具⾥的参数⼀致.
}
response = requests.post(url,data=data)
dic_obj = response.json() # 返回值是json 那就可以直接解析成json
# print(dic_obj) # 拿到返回字典中的内容
# print(dic_obj['data'][0]["v"])
for mean in dic_obj["data"]:
    print(mean["v"])
```

![图片](https://s2.loli.net/2023/06/13/DvUKLzusS86tOok.png)





案例4：豆瓣电影ajax页面提取



是不是很顺⼿呢? 还有⼀些⽹站在进⾏请求的时候会校验你的客户端设备型号. ⽐如, 我们抓取⾖瓣电影，我们构造了一个字典params来请求：网址是豆瓣电影分类排行榜 - 爱情片，你会发现，往下划的过程中一直都会动态加载：

```python
import requests

url = "https://movie.douban.com/j/chart/top_list"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3884.400 QQBrowser/10.8.4560.400'
}
for i in range(0,100,20):
    params ={
        'type': '13',
        'interval_id': '100:90',
        'action': '',
        'start': f'{i}',
        'limit': '20'
}
    response = requests.get(url,params=params,headers=headers)
    dic_obj= response.json()
    for dic in dic_obj:
        print(dic['title'])
```



案例5：爬取腾讯新闻24小时最新内容，



这个也是需要构造一个params字典，然后解析json爬取，一般动态加载的都是用的这种方法：

```python
import requests

url = "https://i.news.qq.com/trpc.qqnews_web.kv_srv.kv_srv_http_proxy/list"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3884.400 QQBrowser/10.8.4560.400'
}
for i in range(0,100,20):
    params = {
        'sub_srv_id': '24hours',
        'srv_id': 'pc',
        'offset': f'{i}',
        'limit': '20',
        'strategy': '1',
        'ext': '{"pool":["top"],"is_filter":7,"check_type":true}'
    }
    response = requests.get(url, params=params, headers=headers)
    dic_obj = response.json()
    # print(dic_obj)
    for dic in dic_obj["data"]["list"]:
        print(dic['title'],dic["url"])
```