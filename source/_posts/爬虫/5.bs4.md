---
title: 5.bs4解析库
cover: 'https://s2.loli.net/2023/06/24/BqZtjYboSVwIfxL.png'
tags: bs4解析库
categories: 爬虫
top_img: 'https://s2.loli.net/2023/06/24/BqZtjYboSVwIfxL.png'
abbrlink: e9ace224
---
### bs4解析库

今天来继续介绍一个爬虫的解析库，和前面说过的xpath，和正则表达式的是同样的原理，就是把所需要的信息从网页中解析出来。

我们先拿一小段代码来做一下学习，这是一段html代码，接下来需要从中找出我们所需要的文本和链接等相关信息。

```python
<html lang="en">
<head>
	<meta charset="UTF-8" />
	<title>测试bs4</title>
</head>
<body>
	<div>
		<p>百里守约</p>
	</div>
	<div class="song">
		<p>李清照</p>
		<p>王安石</p>
		<p>苏轼</p>
		<p>柳宗元</p>
		<a href="http://www.song.com/" title="赵匡胤" target="_self">
			<span>this is span</span>
		宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱</a>
		<a href="" class="du">总为浮云能蔽日,长安不见使人愁</a>
		<img src="http://www.baidu.com/meinv.jpg" alt="" />
	</div>
	<div class="tang">
		<ul>
			<li><a href="百度一下，你就知道" title="qing">清明时节雨纷纷,路上行人欲断魂,借问酒家何处有,牧童遥指杏花村</a></li>
			<li><a href="网易" title="qin">秦时明月汉时关,万里长征人未还,但使龙城飞将在,不教胡马度阴山</a></li>
			<li><a href="126网易免费邮--你的专业电子邮" alt="qi">岐王宅里寻常见,崔九堂前几度闻,正是江南好风景,落花时节又逢君</a></li>
			<li><a href="home.sina.com" class="du">杜甫</a></li>
			<li><a href="Awesome Coming Soon Widget Responsive Widget" class="du">杜牧</a></li>
			<li><b>杜小月</b></li>
			<li><i>度蜜月</i></li>
			<li><a href="http://www.haha.com" id="feng">凤凰台上凤凰游,凤去台空江自流,吴宫花草埋幽径,晋代衣冠成古丘</a></li>
		</ul>
	</div>
</body>
</html>
```



存储到一个文件里，然后通过打开本地文件的操作，就可以开始执行了。

前提是你得先在终端环境中安装这个库。

pip intall bs4

```python
from bs4 import BeautifulSoup

#本地的html
fp = open(r"D:\BaiduNetdiskDownload\爬虫课件\爬虫课件\第三章：数据解析\test.html","r",encoding="utf-8")
soup = BeautifulSoup(fp,"lxml")
# print(soup)  #返回这个网页的源代码
# print(soup.a)  #返回第一个a标签的内容
# print(soup.div) #返回第一个div 的内容
# print(soup.find("div"))  #返回第一个div 的内容
# print(soup.find("div",class_="song"))   #返回class = "song" 的div的内容
# print(soup.find_all("a"))  #返回所有a标签的内容
# print(soup.find("div",class_="song").a.get("href"))  #返回链接http://www.song.com/
# print(soup.find("div",class_="song").img.get("src")) 返回链接：http://www.baidu.com/meinv.jpg
print(soup.find("div",class_="song").a.text) # 返回文本
# print(soup.select('.tang'))   返回class = "tang" 的div的内容
print(soup.select(".tang > ul > li > a")[0].text)   # 返回tang下的第一个标签的内容
print(soup.select(".tang > ul > li > a")[0]['href']) # 返回tang下的第一个标签的链接
```



学习完这个，我们就可以实战了， 第一个案例，获取优美图库（https://www.umeitu.com/bizhitupian/huyanbizhi）的壁纸，并下载高清大图，大图在点击图片后的详情页，所以我们需要两次请求。

![图片](https://s2.loli.net/2023/06/13/w7Hi6bDNZJzaRKC.png)





```
import requests
from bs4 import BeautifulSoup
url ='https://www.umeitu.com/bizhitupian/huyanbizhi/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3884.400 QQBrowser/10.8.4560.400'}
page_list = requests.get(url,headers = headers)
page_list.encoding = "utf-8"
html = page_list.text

# 列表页链接
soup = BeautifulSoup(html,"lxml")
urls = soup.find_all(class_="TypeBigPics")
for page_url in urls:
    page_url = 'https://www.umeitu.com/' + page_url.get('href')
    # print(url)

#获取子页面链接
    page_content = requests.get(page_url,headers = headers)
    html1 = page_content.text
    soup1 = BeautifulSoup(html1,"lxml")
    src = soup1.find("p",align="center")

    img = src.img.get('src')

    # 下载图片
    img_resp = requests.get(img)
    # print(img_resp.content)
    img_name =img.split("/")[-1]
    with open(img_name,"wb") as f:
        f.write(img_resp.content)
```

这里需要注意的是，第一次请求列表页，用的soup.find_all，然后再循环，第二次直接soup1.find（）这样请求的就是单页面。

然后图片就下载到本地了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/uR1mbzfuAWzoPTgjMEpT4iabt2zoRvhsacNhukvUXrbg4MHIiacEVBBG2KLjtT7w9I5JB9aCEGG6EUsscH9Sm24Q/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)





除了下载图片，常见的应用还有下载文本。这次用soup.select方法下载一本小说。

```python
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import requests
from bs4 import BeautifulSoup
if __name__ == "__main__":
    headers ={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3884.400 QQBrowser/10.8.4560.400'}
    url = "http://www.shicimingju.com/book/sanguoyanyi.html"
    page_text = requests.get(url,headers = headers).text.encode("ISO-8859-1")

    soup = BeautifulSoup(page_text,"lxml")
    li_list = soup.select(".book-mulu > ul > li")
    fp = open("sanguo.txt","w",encoding="utf-8")
    for li in li_list:
        title= li.a.string
        detail_url = "https://www.shicimingju.com/"+li.a["href"]
        detail_page_text = requests.get(detail_url,headers= headers).text.encode("ISO-8859-1")

        detail_soup = BeautifulSoup(detail_page_text,"lxml")
        div_tag = detail_soup.find("div",class_ = "card bookmark-list")
        content = div_tag.text
        # print(content)
        fp.write(title+":"+content+"\n")
        print(title,"爬取成功")
```

![图片](https://s2.loli.net/2023/06/13/vAcahJLy6GndHEz.png)



小说就下载好了。感觉这个网站是个鸡肋网站，第一段看着也不是三国呀，就当练习了，毕竟这个网站也没有做反爬。