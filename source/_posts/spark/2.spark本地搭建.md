---
title: 2.Spark开发环境搭建
cover: 'https://s2.loli.net/2023/03/24/TpxC8EFRsB63M5y.jpg'
tags: Spark
categories: Spark
top_img: 'https://s2.loli.net/2023/03/24/TpxC8EFRsB63M5y.jpg'
abbrlink: 5af44e45
---
# Spark开发环境搭建

## 一、安装Spark

### 1.1 下载并解压

官方下载地址：http://spark.apache.org/downloads.html ，选择 Spark 版本和对应的 Hadoop 版本后再下载：

![img](https://s2.loli.net/2023/03/27/6MDYVQnsZGoibey.png)

解压安装包：

```shell
tar -zxvf  spark-2.2.3-bin-hadoop2.6.tgz
```

### 1.2 配置环境变量

```shell
vim /etc/profile
```

添加环境变量：

```shell
export SPARK_HOME=/usr/app/spark-2.2.3-bin-hadoop2.6
export  PATH=${SPARK_HOME}/bin:$PATH
```

使得配置的环境变量立即生效：

```shell
source /etc/profile
```

### 1.3 Local模式

Local 模式是最简单的一种运行方式，它采用单节点多线程方式运行，不用部署，开箱即用，适合日常测试开发。

```shell
# 启动spark-shell
spark-shell --master local[2]
```

- **local**：只启动一个工作线程；
- **local[k]**：启动 k 个工作线程；
- **local[\*]**：启动跟 cpu 数目相同的工作线程数。

![img](https://s2.loli.net/2023/03/27/SEaoFhGn8BWT57Z.png)



进入 spark-shell 后，程序已经自动创建好了上下文 `SparkContext`，等效于执行了下面的 Scala 代码：

```shell
val conf = new SparkConf().setAppName("Spark shell").setMaster("local[2]")
val sc = new SparkContext(conf)
```

## 二、词频统计案例

安装完成后可以先做一个简单的词频统计例子，感受 spark 的魅力。准备一个词频统计的文件样本 `wc.txt`，内容如下：

```shell
hadoop,spark,hadoop
spark,flink,flink,spark
hadoop,hadoop
```

在 scala 交互式命令行中执行如下 Scala 语句：

```shell
val file = spark.sparkContext.textFile("file:///usr/app/wc.txt")
val wordCounts = file.flatMap(line => line.split(",")).map((word => (word, 1))).reduceByKey(_ + _)
wordCounts.collect
```

执行过程如下，可以看到已经输出了词频统计的结果：

![img](https://s2.loli.net/2023/03/27/kOVWGFjteChRAuc.png)

同时还可以通过 Web UI 查看作业的执行情况，访问端口为 `4040`：

![img](https://s2.loli.net/2023/03/27/OhUJGv6kWKNfHe4.png)